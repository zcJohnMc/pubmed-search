# pubmed_search_core.py
# æ ¸å¿ƒPubMedæœç´¢åŠŸèƒ½æ¨¡å—

import requests
import xml.etree.ElementTree as ET
import time
import re
import os
import json
import sqlite3
from datetime import datetime

# è®¾ç½®APIå¯†é’¥å’ŒåŸºç¡€URL (PubMed E-utilities)
PUBMED_API_KEY = os.environ.get("PUBMED_API_KEY", "b6a22ac9a183cabddf8a38046641c2378308")
BASE_URL = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/"

# OpenRouter API Configuration
OPENROUTER_API_KEY = os.environ.get("OPENROUTER_API_KEY", "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")
OPENROUTER_API_URL = "https://openrouter.ai/api/v1/chat/completions"
YOUR_SITE_URL = "http://localhost:5000"
YOUR_SITE_NAME = "AI PubMed Query Tool"

# æ•°æ®åº“é…ç½®
import os
DATABASE_PATH = os.environ.get("DATABASE_PATH", os.path.join(os.path.dirname(os.path.abspath(__file__)), "pubmed_search_history.db"))

# å®šä¹‰ä¸»åˆŠå’Œå­åˆŠåˆ—è¡¨ (32ä¸ªæœŸåˆŠ)
MAIN_JOURNALS = {
    "Nature Reviews Genetics": ["Nature Reviews Genetics"],
    "Nature Structural & Molecular Biology": ["Nature Structural & Molecular Biology", "Nature Structural and Molecular Biology"],
    "Molecular Cell": ["Molecular Cell"],
    "Genome Biology": ["Genome Biology"],
    "Epigenetics & Chromatin": ["Epigenetics & Chromatin", "Epigenetics and Chromatin"],
    "Clinical Epigenetics": ["Clinical Epigenetics"],
    "Epigenetics": ["Epigenetics"],
    "Nature": ["Nature"],
    "Science": ["Science"],
    "Cell": ["Cell"],
    "Nature Genetics": ["Nature Genetics"],
    "Cell Reports": ["Cell Reports"],
    "Nature Communications": ["Nature Communications"],
    "Science Advances": ["Science Advances"],
    "Cancer Discovery": ["Cancer Discovery"],
    "Cell Metabolism": ["Cell Metabolism"],
    "Journal of Clinical Investigation": ["Journal of Clinical Investigation", "JCI"],
    "Oncogene": ["Oncogene"],
    "Cancer Research": ["Cancer Research"],
    "Clinical Cancer Research": ["Clinical Cancer Research"],
    "Nature Reviews Cancer": ["Nature Reviews Cancer"],
    "Bioinformatics": ["Bioinformatics"],
    "PLOS Computational Biology": ["PLOS Computational Biology", "PLoS Computational Biology"],
    "Briefings in Bioinformatics": ["Briefings in Bioinformatics"],
    "Nucleic Acids Research": ["Nucleic Acids Research"],
    "Nature Machine Intelligence": ["Nature Machine Intelligence"],
    "Cell Systems": ["Cell Systems"],
    "IEEE/ACM Trans. Comp. Bio. & Bioinf.": ["IEEE/ACM Trans. Comp. Bio. & Bioinf.", "IEEE/ACM Transactions on Computational Biology and Bioinformatics"],
    "Journal of Biomedical Informatics": ["Journal of Biomedical Informatics"],
    "Artificial Intelligence in Medicine": ["Artificial Intelligence in Medicine"],
    "Patterns": ["Patterns"],
    "Database (Biol. Databases & Curation)": ["Database (Biol. Databases & Curation)", "Database"],
    "GigaScience": ["GigaScience"]
}

SUBSIDIARY_PATTERNS = [
    r"Nature\s+[A-Z]", r"Cell\s+[A-Z]", r"Science\s+[A-Z]",
    r"Lancet\s+[A-Z]", r"JAMA\s+[A-Z]", r"BMJ\s+[A-Z]"
]

JOURNAL_IMPACT_FACTORS = {
    "Nature Reviews Genetics": 39.1, "Nat Rev Genet": 39.1,
    "Nature Structural & Molecular Biology": 12.5, "Nature Structural and Molecular Biology": 12.5,
    "Molecular Cell": 14.5, "Mol Cell": 14.5,
    "Genome Biology": 10.1,
    "Epigenetics & Chromatin": 4.2, "Epigenetics and Chromatin": 4.2,
    "Clinical Epigenetics": 4.8,
    "Epigenetics": 2.9,
    "Nature": 50.5,
    "Science": 44.7,
    "Cell": 45.5,
    "Nature Genetics": 31.7, "Nat Genet": 31.7,
    "Cell Reports": 7.5, "Cell Rep": 7.5,
    "Nature Communications": 14.7, "Nat Commun": 14.7,
    "Science Advances": 11.7, "Sci Adv": 11.7,
    "Cancer Discovery": 29.7,
    "Cell Metabolism": 27.7, "Cell Metab": 27.7,
    "Journal of Clinical Investigation": 13.3, "J Clin Invest": 13.3, "JCI": 13.3,
    "Oncogene": 6.9,
    "Cancer Research": 12.5, "Cancer Res": 12.5,
    "Clinical Cancer Research": 10.0, "Clin Cancer Res": 10.0,
    "Nature Reviews Cancer": 72.5, "Nat Rev Cancer": 72.5,
    "Bioinformatics": 4.4,
    "PLOS Computational Biology": 3.8, "PLoS Computational Biology": 3.8, "PLoS Comput Biol": 3.8,
    "Briefings in Bioinformatics": 6.8, "Brief Bioinform": 6.8,
    "Nucleic Acids Research": 16.7, "Nucleic Acids Res": 16.7,
    "Nature Machine Intelligence": 18.8, "Nat Mach Intell": 18.8,
    "Cell Systems": 9.0, "Cell Syst": 9.0,
    "IEEE/ACM Trans. Comp. Bio. & Bioinf.": 3.6, "IEEE/ACM Transactions on Computational Biology and Bioinformatics": 3.6,
    "Journal of Biomedical Informatics": 4.0, "J Biomed Inform": 4.0,
    "Artificial Intelligence in Medicine": 6.1, "Artif Intell Med": 6.1,
    "Patterns": 6.7,
    "Database (Biol. Databases & Curation)": 3.4, "Database": 3.4,
    "GigaScience": 11.8
}

def init_database():
    """åˆå§‹åŒ–SQLiteæ•°æ®åº“"""
    try:
        conn = sqlite3.connect(DATABASE_PATH)
        cursor = conn.cursor()
        
        # åˆ›å»ºæœç´¢å†å²è¡¨
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS search_history (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                search_date TEXT NOT NULL,
                user_topic TEXT,
                ai_generated_query TEXT,
                final_query TEXT,
                journal_filter TEXT,
                year_range TEXT,
                min_score REAL,
                article_types TEXT,
                total_results INTEGER,
                filtered_results INTEGER,
                search_parameters TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')

        # ä¸ºç°æœ‰æ•°æ®åº“æ·»åŠ article_typeså­—æ®µï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
        try:
            cursor.execute('ALTER TABLE search_history ADD COLUMN article_types TEXT')
        except sqlite3.OperationalError:
            # å­—æ®µå·²å­˜åœ¨ï¼Œå¿½ç•¥é”™è¯¯
            pass
        
        # åˆ›å»ºæ–‡ç« è¯¦æƒ…è¡¨
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS articles (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                search_id INTEGER,
                pmid TEXT NOT NULL,
                title TEXT,
                journal TEXT,
                journal_abbr TEXT,
                year TEXT,
                volume TEXT,
                issue TEXT,
                pages TEXT,
                doi TEXT,
                abstract TEXT,
                authors TEXT,
                article_types TEXT,
                keywords TEXT,
                citation TEXT,
                pubmed_url TEXT,
                impact_factor REAL,
                score REAL,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (search_id) REFERENCES search_history (id),
                UNIQUE(search_id, pmid)
            )
        ''')
        
        # åˆ›å»ºç´¢å¼•ä»¥æé«˜æŸ¥è¯¢æ€§èƒ½
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_search_date ON search_history(search_date)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_pmid ON articles(pmid)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_search_id ON articles(search_id)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_score ON articles(score)')
        
        conn.commit()
        conn.close()
        print("âœ… æ•°æ®åº“åˆå§‹åŒ–å®Œæˆ")
        
    except Exception as e:
        print(f"âŒ æ•°æ®åº“åˆå§‹åŒ–å¤±è´¥: {e}")

def generate_pubmed_query_with_ai(user_topic):
    """
    ä½¿ç”¨AIç”ŸæˆPubMedæœç´¢æŸ¥è¯¢å­—ç¬¦ä¸²
    """
    prompt_xml_md = f"""
<prompt_instructions>
  <role>You are an AI assistant specialized in crafting comprehensive and inclusive PubMed search queries for biomedical research.</role>
  <task>
    Given a user's research topic, generate a PubMed ESearch query that prioritizes RECALL over PRECISION.
    The goal is to capture as many potentially relevant articles as possible, even if some less relevant ones are included.
    It's better to retrieve more articles and let the user filter them, rather than miss important research.
  </task>
  
  <search_philosophy>
    <principle>INCLUSIVE over EXCLUSIVE: Cast a wide net to avoid missing relevant research</principle>
    <principle>BROAD over NARROW: Use OR operators liberally to expand search scope</principle>
    <principle>FLEXIBLE over RIGID: Include variations, synonyms, and related concepts</principle>
    <principle>COMPREHENSIVE over PRECISE: Better to include extra articles than miss key ones</principle>
  </search_philosophy>

  <output_format>
    Return **ONLY** the generated PubMed search string.
    Do not include explanations, introductory text, apologies, or markdown formatting.
    The string should be directly usable as the `term` parameter in a PubMed ESearch API call.

    Example of inclusive output:
    `(telomere OR telomeres OR "telomere length" OR "telomeric DNA" OR telomerase) AND (aging OR ageing OR longevity OR "life span" OR "health span" OR senescence OR "cellular aging")`
  </output_format>

  <guidelines>
    <guideline>1. **Identify ALL Related Concepts**: Extract not just core concepts, but also related, adjacent, and peripheral concepts that researchers might use.</guideline>
    
    <guideline>2. **Maximize Synonym Coverage**: For each concept, include:
        - Scientific terms AND common terms
        - American AND British spellings (e.g., aging/ageing, tumor/tumour)
        - Abbreviations AND full forms
        - Plural AND singular forms
        - Alternative phrasings and expressions
    </guideline>
    
    <guideline>3. **Liberal Use of OR Operators**: 
        - Group synonyms and related terms with OR within parentheses
        - Use OR to connect different ways researchers might describe the same concept
        - Prefer OR over AND when connecting related but distinct concepts
    </guideline>
    
    <guideline>4. **Strategic Use of AND Operators**:
        - Only use AND to connect truly different concept groups
        - Minimize the number of AND connections to avoid over-restriction
        - Consider if concepts could appear in different contexts within the same study
    </guideline>
    
    <guideline>5. **Field Tag Strategy**:
        - Use [tiab] (Title/Abstract) for most terms to capture comprehensive coverage
        - Use [MeSH Terms] sparingly and always with OR alternatives
        - Avoid restrictive field tags that might exclude relevant articles
        - Example: `(diabetes[tiab] OR "diabetes mellitus"[MeSH Terms] OR diabetic[tiab])`
    </guideline>
    
    <guideline>6. **Handle Ambiguous Topics Broadly**:
        - For vague topics, interpret them in multiple possible ways
        - Include both specific and general terms
        - Consider different research contexts and methodologies
        - Example: For "cancer treatment" include: therapy, treatment, intervention, management, care, etc.
    </guideline>
    
    <guideline>7. **Comparative and Qualitative Queries**:
        - For "X vs Y" topics, search for articles mentioning either X OR Y (not necessarily comparing them)
        - For "effectiveness" questions, include: efficacy, effectiveness, outcome, benefit, impact, effect, result
        - For "safety" questions, include: safety, adverse, side effect, toxicity, harm, risk
    </guideline>
    
    <guideline>8. **Temporal and Methodological Inclusivity**:
        - Include terms for different study types: clinical trial, observational, review, meta-analysis, case study
        - Include terms for different populations: human, animal, in vitro, in vivo (unless specifically excluded)
        - Avoid date restrictions in the query itself
    </guideline>
    
    <guideline>9. **Language and Cultural Variations**:
        - Include international terminology variations
        - Consider how different research communities might describe the same phenomenon
        - Include both technical and clinical terminology
    </guideline>
    
    <guideline>10. **Error-Tolerant Construction**:
        - Structure queries to be resilient to minor variations in terminology
        - Use broad categorical terms alongside specific ones
        - Avoid overly complex nested boolean logic that might exclude edge cases
    </guideline>
  </guidelines>

  <examples>
    <example>
      <topic>ç«¯ç²’ä¸è¡°è€</topic>
      <inclusive_query>(telomere OR telomeres OR "telomere length" OR "telomere shortening" OR telomerase OR "telomerase activity" OR "telomeric DNA") AND (aging OR ageing OR longevity OR "life span" OR "health span" OR senescence OR "cellular aging" OR "age-related" OR elderly OR geriatric)</inclusive_query>
    </example>
    
    <example>
      <topic>ç³–å°¿ç—…æ²»ç–—</topic>
      <inclusive_query>(diabetes OR diabetic OR "diabetes mellitus" OR "type 2 diabetes" OR "type 1 diabetes" OR hyperglycemia OR hyperglycaemia) AND (treatment OR therapy OR management OR intervention OR care OR medication OR drug OR insulin OR metformin)</inclusive_query>
    </example>
    
    <example>
      <topic>ç™Œç—‡å…ç–«ç–—æ³•æ•ˆæœ</topic>
      <inclusive_query>(cancer OR tumor OR tumour OR neoplasm OR malignancy OR oncology) AND (immunotherapy OR "immune therapy" OR "checkpoint inhibitor" OR "CAR-T" OR "immune checkpoint" OR immunology) AND (efficacy OR effectiveness OR outcome OR response OR survival OR benefit OR result OR impact)</inclusive_query>
    </example>
  </examples>
</prompt_instructions>

## User's Research Topic:
<user_topic>
{user_topic}
</user_topic>

## Generated Inclusive PubMed Search Query:
"""

    headers = {
        "Authorization": f"Bearer {OPENROUTER_API_KEY}",
        "Content-Type": "application/json",
        "HTTP-Referer": YOUR_SITE_URL,
        "X-Title": YOUR_SITE_NAME,
    }
    
    data = {
        "model": "anthropic/claude-4.5-sonnet",
        "messages": [{"role": "user", "content": prompt_xml_md}],
        "temperature": 0.4,  # é™ä½æ¸©åº¦ä»¥è·å¾—æ›´ä¸€è‡´çš„ç»“æœ
        "top_p": 0.8,        # ç¨å¾®é™ä½ä»¥æé«˜è´¨é‡
    }

    print("\nğŸ¤– æ­£åœ¨ä½¿ç”¨AIç”ŸæˆåŒ…å®¹æ€§PubMedæŸ¥è¯¢...")
    try:
        response = requests.post(OPENROUTER_API_URL, headers=headers, data=json.dumps(data), timeout=60)
        response.raise_for_status()
        
        response_json = response.json()
        
        if response_json.get("choices") and len(response_json["choices"]) > 0:
            ai_content = response_json["choices"][0].get("message", {}).get("content", "")
            # æ¸…ç†å“åº”å†…å®¹
            ai_content_cleaned = ai_content.strip()
            if ai_content_cleaned.lower().startswith("```pubmed"):
                ai_content_cleaned = ai_content_cleaned[len("```pubmed"):]
            elif ai_content_cleaned.lower().startswith("```"):
                ai_content_cleaned = ai_content_cleaned[len("```"):]

            if ai_content_cleaned.lower().endswith("```"):
                ai_content_cleaned = ai_content_cleaned[:-len("```")]
            
            return ai_content_cleaned.strip()
        else:
            print("âŒ AIå“åº”æ ¼å¼é”™è¯¯: æœªæ‰¾åˆ°choicesæˆ–choicesæ•°ç»„ä¸ºç©º")
            return None
            
    except requests.exceptions.RequestException as e:
        print(f"âŒ è°ƒç”¨OpenRouter APIé”™è¯¯: {e}")
        return None
    except Exception as e:
        print(f"âŒ AIæŸ¥è¯¢ç”Ÿæˆæ—¶å‘ç”Ÿæ„å¤–é”™è¯¯: {e}")
        return None

def search_pubmed_with_simplified_query(original_query, journal=None, min_year=None, max_year=None, main_journals_only=True):
    """å½“åŸæŸ¥è¯¢è¿‡é•¿æ—¶ï¼Œä½¿ç”¨ç®€åŒ–æŸ¥è¯¢ä½œä¸ºåå¤‡æ–¹æ¡ˆ"""
    print("ğŸ”„ æ­£åœ¨ç®€åŒ–æŸ¥è¯¢ä»¥é¿å…URLè¿‡é•¿é”™è¯¯...")
    
    # ç®€åŒ–ç­–ç•¥ï¼šæå–æ ¸å¿ƒå…³é”®è¯
    simplified_query = simplify_query(original_query)
    print(f"ğŸ“ ç®€åŒ–åçš„æŸ¥è¯¢: {simplified_query}")
    
    # å¤„ç†æœŸåˆŠè¿‡æ»¤
    search_query = simplified_query
    if journal:
        journals_input_list = journal.replace("ã€", ",").replace("ï¼Œ", ",").split(",")
        journals_input_list = [j.strip() for j in journals_input_list if j.strip()]
        journal_filter_parts = []
        
        if main_journals_only:
            matched_main_journal_variants = set()
            for j_input in journals_input_list:
                found_main = False
                for main_name_key, variants_list in MAIN_JOURNALS.items():
                    if main_name_key.lower() == j_input.lower() or j_input.lower() in [v.lower() for v in variants_list]:
                        for v in variants_list:
                            matched_main_journal_variants.add(f'"{v}"[journal]')
                        found_main = True
                        break
                
                if not found_main:
                    journal_filter_parts.append(f'"{j_input}"[journal]')
            
            if matched_main_journal_variants:
                journal_filter_parts.extend(list(matched_main_journal_variants))
        else:
            for j_input in journals_input_list:
                journal_filter_parts.append(f'"{j_input}"[journal]')
        
        if journal_filter_parts:
            journal_query_segment = " OR ".join(journal_filter_parts)
            search_query += f" AND ({journal_query_segment})"
    
    # å¤„ç†å¹´ä»½è¿‡æ»¤
    if min_year and max_year:
        search_query += f" AND {min_year}:{max_year}[pdat]"
    elif min_year:
        search_query += f" AND {min_year}:[pdat]"
    elif max_year:
        search_query += f" AND :{max_year}[pdat]"
    
    # ä½¿ç”¨POSTè¯·æ±‚æ‰§è¡Œç®€åŒ–æŸ¥è¯¢
    search_url = BASE_URL + "esearch.fcgi"
    search_params = {
        "db": "pubmed", 
        "term": search_query,
        "retmax": 0,
        "usehistory": "y", 
        "api_key": PUBMED_API_KEY
    }
    
    try:
        response = requests.post(search_url, data=search_params, timeout=30)
        response.raise_for_status()
        root = ET.fromstring(response.content)
        
        error_elem = root.find("ERROR")
        if error_elem is not None and error_elem.text:
            print(f"âŒ ç®€åŒ–æŸ¥è¯¢ä¹Ÿå¤±è´¥: {error_elem.text}")
            return {"pmids": [], "web_env": None, "query_key": None, "total_count": 0}
        
        count_elem = root.find("Count")
        total_count = int(count_elem.text) if count_elem is not None else 0
        print(f"ğŸ“Š ç®€åŒ–æŸ¥è¯¢æ‰¾åˆ° {total_count} ç¯‡ç›¸å…³æ–‡ç« ")
        
        if total_count == 0:
            return {"pmids": [], "web_env": None, "query_key": None, "total_count": 0}
        
        # è·å–PMIDs
        search_params["retmax"] = min(total_count, 10000)
        response = requests.post(search_url, data=search_params, timeout=60)
        response.raise_for_status()
        root = ET.fromstring(response.content)
        
        web_env_elem = root.find("WebEnv")
        query_key_elem = root.find("QueryKey")
        
        if web_env_elem is None or query_key_elem is None:
            id_list_direct = [id_elem.text for id_elem in root.findall(".//IdList/Id")]
            if id_list_direct:
                return {"pmids": id_list_direct, "web_env": None, "query_key": None, "total_count": total_count}
            return {"pmids": [], "web_env": None, "query_key": None, "total_count": 0}
        
        web_env = web_env_elem.text
        query_key = query_key_elem.text
        id_list = [id_elem.text for id_elem in root.findall(".//IdList/Id")]
        
        print(f"âœ… ç®€åŒ–æŸ¥è¯¢æˆåŠŸè·å– {len(id_list)} ç¯‡æ–‡ç« çš„PMIDs")
        return {"pmids": id_list, "web_env": web_env, "query_key": query_key, "total_count": total_count}
        
    except Exception as e:
        print(f"âŒ ç®€åŒ–æŸ¥è¯¢å¤±è´¥: {e}")
        return {"pmids": [], "web_env": None, "query_key": None, "total_count": 0}

def simplify_query(query):
    """ç®€åŒ–å¤æ‚çš„PubMedæŸ¥è¯¢"""
    # ç§»é™¤è¿‡å¤šçš„ORæ¡ä»¶ï¼Œä¿ç•™æ ¸å¿ƒå…³é”®è¯
    # æå–ä¸»è¦çš„ANDåˆ†ç»„
    and_parts = query.split(" AND ")
    
    simplified_parts = []
    for part in and_parts:
        if part.strip():
            # å¯¹äºæ¯ä¸ªANDéƒ¨åˆ†ï¼Œå¦‚æœåŒ…å«å¾ˆå¤šORæ¡ä»¶ï¼Œåˆ™ç®€åŒ–
            if " OR " in part and part.count(" OR ") > 10:
                # æå–æ‹¬å·å†…çš„å†…å®¹
                if part.startswith("(") and part.endswith(")"):
                    inner_content = part[1:-1]
                    or_terms = [term.strip() for term in inner_content.split(" OR ")]
                    
                    # ä¿ç•™å‰5ä¸ªæœ€é‡è¦çš„æœ¯è¯­
                    important_terms = []
                    for term in or_terms[:15]:  # åªå–å‰15ä¸ªæœ¯è¯­
                        if not ("[MeSH Terms]" in term and len(important_terms) > 5):
                            important_terms.append(term)
                        if len(important_terms) >= 8:  # é™åˆ¶ä¸º8ä¸ªæœ¯è¯­
                            break
                    
                    simplified_part = "(" + " OR ".join(important_terms) + ")"
                    simplified_parts.append(simplified_part)
                else:
                    simplified_parts.append(part)
            else:
                simplified_parts.append(part)
    
    simplified_query = " AND ".join(simplified_parts)
    
    # å¦‚æœä»ç„¶å¤ªé•¿ï¼Œè¿›ä¸€æ­¥ç®€åŒ–
    if len(simplified_query) > 1000:
        # åªä¿ç•™å‰ä¸¤ä¸ªANDéƒ¨åˆ†
        and_parts = simplified_query.split(" AND ")
        simplified_query = " AND ".join(and_parts[:2])
    
    return simplified_query

def is_main_journal(journal_name):
    """æ£€æŸ¥æœŸåˆŠæ˜¯å¦ä¸ºä¸»åˆŠ"""
    for main_journal_variants in MAIN_JOURNALS.values():
        if any(variant.lower() == journal_name.lower() for variant in main_journal_variants):
            return True
    
    for pattern in SUBSIDIARY_PATTERNS:
        if re.search(pattern, journal_name, re.IGNORECASE):
            for main_journal_variants in MAIN_JOURNALS.values():
                if any(variant.lower() == journal_name.lower() for variant in main_journal_variants):
                    return True
            return False
    return False

def search_pubmed(query, journal=None, min_year=None, max_year=None, main_journals_only=True):
    """æœç´¢PubMedæ–‡ç« ï¼Œè·å–æ‰€æœ‰ç»“æœ"""
    print(f"ğŸ“ åŸå§‹æœç´¢è¯: {query}")
    
    # å¤„ç†ä¸­æ–‡æ ‡ç‚¹ç¬¦å·
    query = query.replace("ï¼Œ", " ").replace("ã€", " ")
    print(f"ğŸ”§ å¤„ç†åçš„æœç´¢è¯: {query}")
    
    search_query = query
    
    # å¤„ç†æœŸåˆŠè¿‡æ»¤
    if journal:
        journals_input_list = journal.replace("ã€", ",").replace("ï¼Œ", ",").split(",")
        journals_input_list = [j.strip() for j in journals_input_list if j.strip()]
        journal_filter_parts = []
        
        if main_journals_only:
            matched_main_journal_variants = set()
            for j_input in journals_input_list:
                found_main = False
                for main_name_key, variants_list in MAIN_JOURNALS.items():
                    if main_name_key.lower() == j_input.lower() or j_input.lower() in [v.lower() for v in variants_list]:
                        for v in variants_list:
                            matched_main_journal_variants.add(f'"{v}"[journal]')
                        found_main = True
                        break
                
                if not found_main:
                    print(f"âš ï¸ è­¦å‘Š: '{j_input}' ä¸åœ¨é¢„å®šä¹‰çš„ä¸»åˆŠåˆ—è¡¨ä¸­ï¼Œä½†å°†æŒ‰å­—é¢æ„æ€æœç´¢")
                    journal_filter_parts.append(f'"{j_input}"[journal]')
            
            if matched_main_journal_variants:
                journal_filter_parts.extend(list(matched_main_journal_variants))
                print(f"ğŸ“š æœŸåˆŠè¿‡æ»¤ (ä¸»åˆŠæ¨¡å¼): {', '.join(journals_input_list)}")
            elif not journal_filter_parts:
                print(f"âš ï¸ è­¦å‘Š: ç”¨æˆ·æŒ‡å®šçš„æœŸåˆŠå‡æœªåŒ¹é…åˆ°é¢„å®šä¹‰çš„ä¸»åˆŠåˆ—è¡¨")
        else:
            for j_input in journals_input_list:
                journal_filter_parts.append(f'"{j_input}"[journal]')
            print(f"ğŸ“š æœŸåˆŠè¿‡æ»¤ (æ‰€æœ‰æœŸåˆŠæ¨¡å¼): {', '.join(journals_input_list)}")
        
        if journal_filter_parts:
            journal_query_segment = " OR ".join(journal_filter_parts)
            search_query += f" AND ({journal_query_segment})"
    
    # å¤„ç†å¹´ä»½è¿‡æ»¤
    if min_year and max_year:
        search_query += f" AND {min_year}:{max_year}[pdat]"
        print(f"ğŸ“… å¹´ä»½èŒƒå›´: {min_year}-{max_year}")
    elif min_year:
        search_query += f" AND {min_year}:[pdat]"
        print(f"ğŸ“… èµ·å§‹å¹´ä»½: {min_year}")
    elif max_year:
        search_query += f" AND :{max_year}[pdat]"
        print(f"ğŸ“… æˆªæ­¢å¹´ä»½: {max_year}")
    
    if not search_query.strip():
        print("âŒ é”™è¯¯ï¼šæœç´¢æŸ¥è¯¢ä¸ºç©ºã€‚è¯·æä¾›æœ‰æ•ˆçš„å…³é”®è¯ã€‚")
        return {"pmids": [], "web_env": None, "query_key": None, "total_count": 0}

    print(f"ğŸ” æœ€ç»ˆæœç´¢æŸ¥è¯¢: {search_query}")
    
    # æ£€æŸ¥æŸ¥è¯¢é•¿åº¦ï¼Œå†³å®šä½¿ç”¨GETè¿˜æ˜¯POST
    search_url = BASE_URL + "esearch.fcgi"
    search_params = {
        "db": "pubmed", 
        "term": search_query,
        "retmax": 0,  # åªè·å–è®¡æ•°
        "usehistory": "y", 
        "api_key": PUBMED_API_KEY
    }
    
    # ä¼°ç®—URLé•¿åº¦ï¼Œå¦‚æœå¤ªé•¿åˆ™ä½¿ç”¨POST
    estimated_url_length = len(search_url) + sum(len(f"{k}={v}&") for k, v in search_params.items())
    use_post = estimated_url_length > 2000  # ä¿å®ˆä¼°è®¡ï¼ŒURLé•¿åº¦è¶…è¿‡2000å­—ç¬¦æ—¶ä½¿ç”¨POST
    
    if use_post:
        print("ğŸ”„ æŸ¥è¯¢è¾ƒé•¿ï¼Œä½¿ç”¨POSTè¯·æ±‚...")
    
    try:
        # é¦–å…ˆè·å–æ€»æ•°
        if use_post:
            response = requests.post(search_url, data=search_params, timeout=30)
        else:
            response = requests.get(search_url, params=search_params, timeout=30)
        
        response.raise_for_status()
        root = ET.fromstring(response.content)
        
        error_elem = root.find("ERROR")
        if error_elem is not None and error_elem.text:
            print(f"âŒ PubMed APIé”™è¯¯: {error_elem.text}")
            return {"pmids": [], "web_env": None, "query_key": None, "total_count": 0}
        
        count_elem = root.find("Count")
        total_count = int(count_elem.text) if count_elem is not None else 0
        print(f"ğŸ“Š æ‰¾åˆ°æ€»è®¡ {total_count} ç¯‡ç›¸å…³æ–‡ç« ")
        
        if total_count == 0:
            return {"pmids": [], "web_env": None, "query_key": None, "total_count": 0}
        
        # ç°åœ¨è·å–æ‰€æœ‰PMIDs
        search_params["retmax"] = min(total_count, 10000)  # PubMed APIé™åˆ¶
        
        if use_post:
            response = requests.post(search_url, data=search_params, timeout=60)
        else:
            response = requests.get(search_url, params=search_params, timeout=60)
        
        response.raise_for_status()
        root = ET.fromstring(response.content)
        
        web_env_elem = root.find("WebEnv")
        query_key_elem = root.find("QueryKey")
        
        if web_env_elem is None or query_key_elem is None:
            print("âš ï¸ æœç´¢å“åº”ä¸­ç¼ºå°‘WebEnvæˆ–QueryKey")
            id_list_direct = [id_elem.text for id_elem in root.findall(".//IdList/Id")]
            if id_list_direct:
                print("ğŸ”„ ä½¿ç”¨ç›´æ¥IDåˆ—è¡¨ä½œä¸ºåå¤‡")
                return {"pmids": id_list_direct, "web_env": None, "query_key": None, "total_count": total_count}
            return {"pmids": [], "web_env": None, "query_key": None, "total_count": 0}
        
        web_env = web_env_elem.text
        query_key = query_key_elem.text
        id_list = [id_elem.text for id_elem in root.findall(".//IdList/Id")]
        
        print(f"âœ… æˆåŠŸè·å– {len(id_list)} ç¯‡æ–‡ç« çš„PMIDsç”¨äºè¯¦æƒ…æå–")
        return {"pmids": id_list, "web_env": web_env, "query_key": query_key, "total_count": total_count}
        
    except requests.exceptions.HTTPError as e:
        if "414" in str(e) or "Request-URI Too Long" in str(e):
            print("âš ï¸ æŸ¥è¯¢è¿‡é•¿ï¼Œå°è¯•ç®€åŒ–æŸ¥è¯¢...")
            # å°è¯•ç®€åŒ–æŸ¥è¯¢çš„åå¤‡æ–¹æ¡ˆ
            return search_pubmed_with_simplified_query(query, journal, min_year, max_year, main_journals_only)
        else:
            print(f"âŒ HTTPé”™è¯¯: {e}")
            return {"pmids": [], "web_env": None, "query_key": None, "total_count": 0}
    except ET.ParseError as e:
        print(f"âŒ XMLè§£æé”™è¯¯: {e}")
        # Log the problematic XML content for debugging
        if 'response' in locals() and response is not None:
            try:
                problematic_xml_content = response.content.decode('utf-8', errors='replace')
                print(f"ğŸ“„ é—®é¢˜XMLå†…å®¹ (å‰1000å­—ç¬¦): {problematic_xml_content[:1000]}")
                # If the error is specific, like line 198, try to log around that area
                lines = problematic_xml_content.splitlines()
                if len(lines) >= 198:
                    start_line = max(0, 198 - 10) # Log 10 lines before
                    end_line = min(len(lines), 198 + 10) # Log 10 lines after
                    print(f"ğŸ“„ é—®é¢˜XMLå†…å®¹ (è¡Œ {start_line + 1} åˆ° {end_line}):")
                    for i in range(start_line, end_line):
                        print(f"{i+1:03d}: {lines[i]}")
            except Exception as log_e:
                print(f"âŒ è®°å½•é—®é¢˜XMLæ—¶å‡ºé”™: {log_e}")
        return {"pmids": [], "web_env": None, "query_key": None, "total_count": 0}
    except Exception as e:
        print(f"âŒ æœç´¢å‡ºé”™: {e}")
        if 'response' in locals() and response is not None:
             print(f"ğŸ“„ å¯èƒ½ç›¸å…³çš„å“åº”çŠ¶æ€ç : {response.status_code}")
        return {"pmids": [], "web_env": None, "query_key": None, "total_count": 0}

def get_element_text_recursive(element):
    """é€’å½’è·å–å…ƒç´ æ–‡æœ¬å†…å®¹"""
    if element is None: 
        return ""
    
    text = element.text or ""
    for child in element:
        text += get_element_text_recursive(child)
        if child.tail:
            text += child.tail
    return text.strip()

def fetch_article_details(pmids=None, web_env=None, query_key=None, main_journals_only=True, batch_size=1000):
    """æ‰¹é‡è·å–æ–‡ç« è¯¦ç»†ä¿¡æ¯ - æ”¹è¿›ç‰ˆæœ¬"""
    print("ğŸ”„ æ­£åœ¨è·å–æ–‡ç« è¯¦ç»†ä¿¡æ¯...")
    
    if not pmids and (not web_env or not query_key):
        print("âŒ é”™è¯¯: å¿…é¡»æä¾›PMIDsæˆ–WebEnv+QueryKey")
        return []
    
    if pmids and len(pmids) == 0 and (not web_env or not query_key):
        print("âŒ æ²¡æœ‰æ‰¾åˆ°æ–‡ç« IDï¼Œæ— æ³•è·å–è¯¦ç»†ä¿¡æ¯")
        return []
    
    fetch_url = BASE_URL + "efetch.fcgi"
    all_articles = []
    
    if pmids:
        total_pmids = len(pmids)
        print(f"ğŸ“„ å‡†å¤‡åˆ†æ‰¹è·å– {total_pmids} ç¯‡æ–‡ç« è¯¦æƒ… (æ¯æ‰¹ {batch_size} ç¯‡)")
        
        # åˆ†æ‰¹å¤„ç†PMIDs
        for i in range(0, total_pmids, batch_size):
            batch_pmids = pmids[i:i+batch_size]
            batch_num = i // batch_size + 1
            total_batches = (total_pmids + batch_size - 1) // batch_size
            
            print(f"â³ æ­£åœ¨å¤„ç†ç¬¬ {batch_num}/{total_batches} æ‰¹ ({len(batch_pmids)} ç¯‡æ–‡ç« )...")
            
            # é‡è¯•æœºåˆ¶
            max_retries = 3
            retry_delay = 2
            
            for retry in range(max_retries):
                try:
                    fetch_params = {
                        "db": "pubmed", 
                        "retmode": "xml", 
                        "api_key": PUBMED_API_KEY,
                        "id": ",".join(batch_pmids)
                    }
                    
                    # Use POST for large batches to avoid URL length limits
                    if len(batch_pmids) > 200:  # Use POST for batches larger than 200
                        response = requests.post(fetch_url, data=fetch_params, timeout=120)
                    else:
                        response = requests.get(fetch_url, params=fetch_params, timeout=120)
                    response.raise_for_status()
                    
                    if not response.content:
                        print(f"âš ï¸ ç¬¬ {batch_num} æ‰¹è·å–åˆ°ç©ºå“åº”")
                        break
                    
                    root = ET.fromstring(response.content)
                    batch_articles = parse_articles_from_xml(root, main_journals_only)
                    all_articles.extend(batch_articles)
                    
                    print(f"âœ… ç¬¬ {batch_num} æ‰¹å®Œæˆï¼Œè·å– {len(batch_articles)} ç¯‡æœ‰æ•ˆæ–‡ç« ")
                    break  # æˆåŠŸåˆ™è·³å‡ºé‡è¯•å¾ªç¯
                    
                except (requests.exceptions.SSLError, requests.exceptions.ConnectionError) as e:
                    if retry < max_retries - 1:
                        print(f"âš ï¸ ç¬¬ {batch_num} æ‰¹ç½‘ç»œé”™è¯¯ï¼Œ{retry_delay}ç§’åé‡è¯• ({retry + 1}/{max_retries})")
                        time.sleep(retry_delay)
                        retry_delay *= 2  # æŒ‡æ•°é€€é¿
                    else:
                        print(f"âŒ ç¬¬ {batch_num} æ‰¹å¤„ç†å¤±è´¥ (å·²é‡è¯•{max_retries}æ¬¡): {e}")
                        
                except Exception as e:
                    print(f"âŒ ç¬¬ {batch_num} æ‰¹å¤„ç†å¤±è´¥: {e}")
                    break
            
            # æ·»åŠ å»¶è¿Ÿä»¥é¿å…APIé™åˆ¶
            if i + batch_size < total_pmids:
                time.sleep(2)  # ç¨å¾®å¢åŠ å»¶è¿Ÿæ—¶é—´ä»¥é€‚åº”æ›´å¤§çš„æ‰¹æ¬¡
    
    elif web_env and query_key:
        # ä½¿ç”¨WebEnv/QueryKeyæ–¹å¼
        print("ğŸ”„ ä½¿ç”¨WebEnv/QueryKeyæ–¹å¼è·å–æ–‡ç« è¯¦æƒ…")
        fetch_params = {
            "db": "pubmed", 
            "retmode": "xml", 
            "api_key": PUBMED_API_KEY,
            "WebEnv": web_env,
            "query_key": query_key,
            "retstart": "0",
            "retmax": "10000"  # æœ€å¤§é™åˆ¶
        }
        
        try:
            response = requests.get(fetch_url, params=fetch_params, timeout=120)
            response.raise_for_status()
            
            if not response.content:
                print("âŒ è·å–åˆ°ç©ºå“åº”")
                return []
            
            root = ET.fromstring(response.content)
            all_articles = parse_articles_from_xml(root, main_journals_only)
            
        except Exception as e:
            print(f"âŒ è·å–æ–‡ç« è¯¦ç»†ä¿¡æ¯æ—¶å‡ºé”™: {e}")
            return []
    
    print(f"ğŸ‰ æ€»å…±æˆåŠŸè·å–å¹¶è§£æ {len(all_articles)} ç¯‡æ–‡ç« çš„è¯¦ç»†ä¿¡æ¯")
    return all_articles

def fetch_article_details_with_progress(pmids=None, web_env=None, query_key=None, 
                                      main_journals_only=True, batch_size=1000, 
                                      progress_callback=None):
    """æ‰¹é‡è·å–æ–‡ç« è¯¦ç»†ä¿¡æ¯ - æ”¯æŒè¿›åº¦å›è°ƒ"""
    print("ğŸ”„ æ­£åœ¨è·å–æ–‡ç« è¯¦ç»†ä¿¡æ¯...")
    
    if not pmids and (not web_env or not query_key):
        print("âŒ é”™è¯¯: å¿…é¡»æä¾›PMIDsæˆ–WebEnv+QueryKey")
        return []
    
    if pmids and len(pmids) == 0 and (not web_env or not query_key):
        print("âŒ æ²¡æœ‰æ‰¾åˆ°æ–‡ç« IDï¼Œæ— æ³•è·å–è¯¦ç»†ä¿¡æ¯")
        return []
    
    fetch_url = BASE_URL + "efetch.fcgi"
    all_articles = []
    
    if pmids:
        total_pmids = len(pmids)
        print(f"ğŸ“„ å‡†å¤‡åˆ†æ‰¹è·å– {total_pmids} ç¯‡æ–‡ç« è¯¦æƒ… (æ¯æ‰¹ {batch_size} ç¯‡)")
        
        # åˆ†æ‰¹å¤„ç†PMIDs
        for i in range(0, total_pmids, batch_size):
            batch_pmids = pmids[i:i+batch_size]
            batch_num = i // batch_size + 1
            total_batches = (total_pmids + batch_size - 1) // batch_size
            
            # æ›´æ–°è¿›åº¦
            if progress_callback:
                progress_callback(i, total_pmids)
            
            print(f"â³ æ­£åœ¨å¤„ç†ç¬¬ {batch_num}/{total_batches} æ‰¹ ({len(batch_pmids)} ç¯‡æ–‡ç« )...")
            
            # é‡è¯•æœºåˆ¶
            max_retries = 3
            retry_delay = 2
            
            for retry in range(max_retries):
                try:
                    fetch_params = {
                        "db": "pubmed", 
                        "retmode": "xml", 
                        "api_key": PUBMED_API_KEY,
                        "id": ",".join(batch_pmids)
                    }
                    
                    # Use POST for large batches to avoid URL length limits
                    if len(batch_pmids) > 200:  # Use POST for batches larger than 200
                        response = requests.post(fetch_url, data=fetch_params, timeout=120)
                    else:
                        response = requests.get(fetch_url, params=fetch_params, timeout=120)
                    response.raise_for_status()
                    
                    if not response.content:
                        print(f"âš ï¸ ç¬¬ {batch_num} æ‰¹è·å–åˆ°ç©ºå“åº”")
                        break
                    
                    root = ET.fromstring(response.content)
                    batch_articles = parse_articles_from_xml(root, main_journals_only)
                    all_articles.extend(batch_articles)
                    
                    print(f"âœ… ç¬¬ {batch_num} æ‰¹å®Œæˆï¼Œè·å– {len(batch_articles)} ç¯‡æœ‰æ•ˆæ–‡ç« ")
                    break  # æˆåŠŸåˆ™è·³å‡ºé‡è¯•å¾ªç¯
                    
                except (requests.exceptions.SSLError, requests.exceptions.ConnectionError) as e:
                    if retry < max_retries - 1:
                        print(f"âš ï¸ ç¬¬ {batch_num} æ‰¹ç½‘ç»œé”™è¯¯ï¼Œ{retry_delay}ç§’åé‡è¯• ({retry + 1}/{max_retries})")
                        time.sleep(retry_delay)
                        retry_delay *= 2  # æŒ‡æ•°é€€é¿
                    else:
                        print(f"âŒ ç¬¬ {batch_num} æ‰¹å¤„ç†å¤±è´¥ (å·²é‡è¯•{max_retries}æ¬¡): {e}")
                        
                except Exception as e:
                    print(f"âŒ ç¬¬ {batch_num} æ‰¹å¤„ç†å¤±è´¥: {e}")
                    break
            
            # æ·»åŠ å»¶è¿Ÿä»¥é¿å…APIé™åˆ¶
            if i + batch_size < total_pmids:
                time.sleep(1)
        
        # æœ€ç»ˆè¿›åº¦æ›´æ–°
        if progress_callback:
            progress_callback(total_pmids, total_pmids)
    
    elif web_env and query_key:
        # ä½¿ç”¨WebEnv/QueryKeyæ–¹å¼
        print("ğŸ”„ ä½¿ç”¨WebEnv/QueryKeyæ–¹å¼è·å–æ–‡ç« è¯¦æƒ…")
        fetch_params = {
            "db": "pubmed", 
            "retmode": "xml", 
            "api_key": PUBMED_API_KEY,
            "WebEnv": web_env,
            "query_key": query_key,
            "retstart": "0",
            "retmax": "10000"  # æœ€å¤§é™åˆ¶
        }
        
        try:
            response = requests.get(fetch_url, params=fetch_params, timeout=120)
            response.raise_for_status()
            
            if not response.content:
                print("âŒ è·å–åˆ°ç©ºå“åº”")
                return []
            
            root = ET.fromstring(response.content)
            all_articles = parse_articles_from_xml(root, main_journals_only)
            
        except Exception as e:
            print(f"âŒ è·å–æ–‡ç« è¯¦ç»†ä¿¡æ¯æ—¶å‡ºé”™: {e}")
            return []
    
    print(f"ğŸ‰ æ€»å…±æˆåŠŸè·å–å¹¶è§£æ {len(all_articles)} ç¯‡æ–‡ç« çš„è¯¦ç»†ä¿¡æ¯")
    return all_articles

def parse_articles_from_xml(root, main_journals_only=True):
    """ä»XMLè§£ææ–‡ç« ä¿¡æ¯"""
    articles = []
    
    for article_elem in root.findall(".//PubmedArticle"):
        try:
            pmid_elem = article_elem.find(".//PMID")
            if pmid_elem is None or not pmid_elem.text: 
                continue
            pmid = pmid_elem.text
            
            title_elem = article_elem.find(".//ArticleTitle")
            title = get_element_text_recursive(title_elem) if title_elem is not None else "æ— æ ‡é¢˜"
            
            journal_title_elem = article_elem.find(".//Journal/Title")
            journal_name_raw = journal_title_elem.text if journal_title_elem is not None and journal_title_elem.text else "æœªçŸ¥æœŸåˆŠ"
            
            # ä¸»åˆŠè¿‡æ»¤
            if main_journals_only:
                is_target_main_journal = False
                for main_variants_list in MAIN_JOURNALS.values():
                    if any(variant.lower() == journal_name_raw.lower() for variant in main_variants_list):
                        is_target_main_journal = True
                        break
                if not is_target_main_journal: 
                    continue
            
            journal_abbr_elem = article_elem.find(".//Journal/ISOAbbreviation")
            journal_abbr = journal_abbr_elem.text if journal_abbr_elem is not None and journal_abbr_elem.text else ""
            
            # è·å–å¹´ä»½
            year_elem = article_elem.find(".//PubDate/Year")
            if year_elem is None or not year_elem.text:
                medline_date_elem = article_elem.find(".//MedlineDate")
                if medline_date_elem is not None and medline_date_elem.text:
                    match = re.search(r"^\d{4}", medline_date_elem.text)
                    year = match.group(0) if match else "æœªçŸ¥å¹´ä»½"
                else: 
                    year = "æœªçŸ¥å¹´ä»½"
            else: 
                year = year_elem.text
            
            # è·å–å·å·ã€æœŸå·ã€é¡µç 
            volume_elem = article_elem.find(".//Volume")
            volume = volume_elem.text if volume_elem is not None and volume_elem.text else ""
            
            issue_elem = article_elem.find(".//Issue")
            issue = issue_elem.text if issue_elem is not None and issue_elem.text else ""
            
            pages_elem = article_elem.find(".//MedlinePgn")
            pages = pages_elem.text if pages_elem is not None and pages_elem.text else ""
            
            # è·å–DOI
            doi = ""
            for art_id in article_elem.findall(".//ArticleId[@IdType='doi']"):
                doi = art_id.text
                break
            
            # è·å–æ‘˜è¦
            abstract_elem = article_elem.find(".//Abstract")
            if abstract_elem is not None:
                abstract_parts_texts = []
                for part_elem in abstract_elem.findall(".//AbstractText"):
                    part_text = get_element_text_recursive(part_elem)
                    if part_text:
                        label = part_elem.get("Label")
                        if label: 
                            abstract_parts_texts.append(f"{label.upper()}: {part_text}")
                        else: 
                            abstract_parts_texts.append(part_text)
                abstract = " ".join(abstract_parts_texts) if abstract_parts_texts else "æ— æ‘˜è¦"
            else: 
                abstract = "æ— æ‘˜è¦"
            
            # è·å–ä½œè€…
            authors = []
            author_list = article_elem.findall(".//Author")
            for author_node in author_list:
                last_name_node = author_node.find("LastName")
                fore_name_node = author_node.find("ForeName")
                author_name = ""
                if fore_name_node is not None and fore_name_node.text: 
                    author_name += fore_name_node.text + " "
                if last_name_node is not None and last_name_node.text: 
                    author_name += last_name_node.text
                if author_name.strip(): 
                    authors.append(author_name.strip())
                elif author_node.find("CollectiveName") is not None and author_node.find("CollectiveName").text:
                    authors.append(author_node.find("CollectiveName").text)
            
            # è·å–æ–‡ç« ç±»å‹å’Œå…³é”®è¯
            article_types = [pt.text for pt in article_elem.findall(".//PublicationTypeList/PublicationType") if pt.text]
            keywords = [kw.text for kw in article_elem.findall(".//KeywordList/Keyword") if kw.text]
            
            # ç”Ÿæˆå¼•ç”¨æ ¼å¼
            citation_authors = ", ".join(authors[:3])
            if len(authors) > 3: 
                citation_authors += ", et al"
            citation = f"{citation_authors}. {title}. {journal_abbr or journal_name_raw}. {year}"
            if volume: 
                citation += f";{volume}"
            if issue: 
                citation += f"({issue})"
            if pages: 
                citation += f":{pages}"
            citation += "."
            if doi: 
                citation += f" doi: {doi}."
            
            # è·å–å½±å“å› å­
            impact_factor = 0.0
            journal_keys_to_try = [journal_name_raw, journal_name_raw.lower()]
            if journal_abbr: 
                journal_keys_to_try.extend([journal_abbr, journal_abbr.lower()])
            
            for key_to_try in journal_keys_to_try:
                if key_to_try in JOURNAL_IMPACT_FACTORS:
                    impact_factor = JOURNAL_IMPACT_FACTORS[key_to_try]
                    break
            
            if impact_factor == 0.0 and journal_name_raw != "æœªçŸ¥æœŸåˆŠ":
                for jf_key, jf_val in JOURNAL_IMPACT_FACTORS.items():
                    if journal_name_raw.lower() == jf_key.lower():
                         impact_factor = jf_val
                         break
            
            # æ„å»ºæ–‡ç« æ•°æ®
            article_data = {
                "pmid": pmid, 
                "title": title, 
                "journal": journal_name_raw, 
                "journal_abbr": journal_abbr,
                "year": year, 
                "volume": volume, 
                "issue": issue, 
                "pages": pages, 
                "doi": doi,
                "abstract": abstract, 
                "authors": authors, 
                "article_types": article_types,
                "keywords": keywords, 
                "citation": citation,
                "pubmed_url": f"https://pubmed.ncbi.nlm.nih.gov/{pmid}/",
                "impact_factor": impact_factor
            }
            articles.append(article_data)
            
        except Exception as e:
            print(f"âŒ è§£ææ–‡ç« PMID {pmid if 'pmid' in locals() else 'Unknown'} æ—¶å‡ºé”™: {e}")
            continue
    
    return articles

def assign_scores_by_if(articles):
    """åŸºäºå½±å“å› å­ä¸ºæ–‡ç« åˆ†é…åˆ†æ•°"""
    print("ğŸ“Š æ­£åœ¨åŸºäºæœŸåˆŠå½±å“å› å­ä¸ºæ–‡ç« åˆ†é…åˆ†æ•°...")
    
    for article in articles:
        score = article.get("impact_factor", 0.0)
        article_types = article.get("article_types", [])
        
        # æ ¹æ®æ–‡ç« ç±»å‹åŠ åˆ†
        if any(re.search("Review", art_type, re.IGNORECASE) for art_type in article_types): 
            score += 8
        if any(re.search(r"Clinical Trial", art_type, re.IGNORECASE) for art_type in article_types) or \
           any(re.search(r"Randomized Controlled Trial", art_type, re.IGNORECASE) for art_type in article_types): 
            score += 7
        if any(re.search(r"Meta-Analysis", art_type, re.IGNORECASE) for art_type in article_types): 
            score += 6
        
        # æ ¹æ®å‘è¡¨å¹´ä»½åŠ åˆ†
        try:
            year_val = int(str(article["year"])[:4])
            current_year = datetime.now().year
            if year_val >= current_year - 2: 
                score += 5
            elif year_val >= current_year - 5: 
                score += 3
        except (ValueError, TypeError): 
            pass
        
        article["score"] = round(score, 2)
    
    # æŒ‰åˆ†æ•°æ’åº
    articles.sort(key=lambda x: x["score"], reverse=True)
    print("âœ… æ–‡ç« è¯„åˆ†å®Œæˆ")
    return articles

def filter_articles(articles, min_score=None):
    """æ ¹æ®æœ€ä½åˆ†æ•°è¿‡æ»¤æ–‡ç« """
    if min_score is None or min_score == 0:
        return articles

    filtered = [article for article in articles if article.get("score", 0) >= min_score]
    print(f"ğŸ” æŒ‰æœ€ä½åˆ†æ•° {min_score} è¿‡æ»¤åï¼Œä¿ç•™ {len(filtered)} ç¯‡æ–‡ç« ")
    return filtered

def filter_articles_by_type(articles, article_types=None):
    """æ ¹æ®æ–‡ç« ç±»å‹è¿‡æ»¤æ–‡ç« """
    if not article_types or 'all' in article_types:
        return articles

    # è¿‡æ»¤æ‰'all'é€‰é¡¹ï¼Œåªä¿ç•™å…·ä½“çš„æ–‡ç« ç±»å‹
    specific_types = [t for t in article_types if t != 'all']
    if not specific_types:
        return articles

    filtered = []
    for article in articles:
        article_type_list = article.get("article_types", [])
        # æ£€æŸ¥æ–‡ç« æ˜¯å¦åŒ…å«ä»»ä½•ä¸€ä¸ªæŒ‡å®šçš„ç±»å‹
        if any(any(selected_type.lower() in art_type.lower() for art_type in article_type_list)
               for selected_type in specific_types):
            filtered.append(article)

    print(f"ğŸ“‹ æŒ‰æ–‡ç« ç±»å‹ {', '.join(specific_types)} è¿‡æ»¤åï¼Œä¿ç•™ {len(filtered)} ç¯‡æ–‡ç« ")
    return filtered

def save_search_to_database(search_params, articles):
    """å°†æœç´¢ç»“æœä¿å­˜åˆ°æ•°æ®åº“"""
    try:
        conn = sqlite3.connect(DATABASE_PATH)
        cursor = conn.cursor()
        
        # æ’å…¥æœç´¢å†å²
        cursor.execute('''
            INSERT INTO search_history
            (search_date, user_topic, ai_generated_query, final_query, journal_filter,
             year_range, min_score, article_types, total_results, filtered_results, search_parameters)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        ''', (
            datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            search_params.get('user_topic', ''),
            search_params.get('ai_generated_query', ''),
            search_params.get('final_query', ''),
            search_params.get('journal_filter', ''),
            search_params.get('year_range', ''),
            search_params.get('min_score', 0.0),
            search_params.get('article_types', 'æ‰€æœ‰ç±»å‹'),
            search_params.get('total_results', 0),
            len(articles),
            json.dumps(search_params, ensure_ascii=False)
        ))
        
        search_id = cursor.lastrowid
        
        # æ’å…¥æ–‡ç« è¯¦æƒ…
        for article in articles:
            cursor.execute('''
                INSERT OR REPLACE INTO articles 
                (search_id, pmid, title, journal, journal_abbr, year, volume, issue, pages, 
                 doi, abstract, authors, article_types, keywords, citation, pubmed_url, 
                 impact_factor, score)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                search_id, article['pmid'], article['title'], article['journal'],
                article.get('journal_abbr', ''), article['year'], article.get('volume', ''),
                article.get('issue', ''), article.get('pages', ''), article.get('doi', ''),
                article['abstract'], json.dumps(article['authors'], ensure_ascii=False),
                json.dumps(article.get('article_types', []), ensure_ascii=False),
                json.dumps(article.get('keywords', []), ensure_ascii=False),
                article['citation'], article['pubmed_url'],
                article.get('impact_factor', 0.0), article.get('score', 0.0)
            ))
        
        conn.commit()
        conn.close()
        print(f"âœ… æœç´¢ç»“æœå·²ä¿å­˜åˆ°æ•°æ®åº“ (æœç´¢ID: {search_id})")
        return search_id
        
    except Exception as e:
        print(f"âŒ ä¿å­˜åˆ°æ•°æ®åº“å¤±è´¥: {e}")
        return None

def get_search_history(limit=20):
    """è·å–æœç´¢å†å²"""
    try:
        conn = sqlite3.connect(DATABASE_PATH)
        cursor = conn.cursor()
        
        cursor.execute('''
            SELECT id, search_date, user_topic, final_query, journal_filter, 
                   year_range, filtered_results, total_results
            FROM search_history 
            ORDER BY created_at DESC 
            LIMIT ?
        ''', (limit,))
        
        history = []
        for row in cursor.fetchall():
            history.append({
                'id': row[0],
                'search_date': row[1],
                'user_topic': row[2],
                'final_query': row[3],
                'journal_filter': row[4],
                'year_range': row[5],
                'filtered_results': row[6],
                'total_results': row[7]
            })
        
        conn.close()
        return history
        
    except Exception as e:
        print(f"âŒ è·å–æœç´¢å†å²å¤±è´¥: {e}")
        return []

def get_search_by_id(search_id):
    """æ ¹æ®IDè·å–æœç´¢ç»“æœ"""
    try:
        conn = sqlite3.connect(DATABASE_PATH)
        cursor = conn.cursor()
        
        # è·å–æœç´¢ä¿¡æ¯
        cursor.execute('''
            SELECT * FROM search_history WHERE id = ?
        ''', (search_id,))
        
        search_info = cursor.fetchone()
        if not search_info:
            return None
        
        # è·å–æ–‡ç« åˆ—è¡¨
        cursor.execute('''
            SELECT * FROM articles WHERE search_id = ? ORDER BY score DESC
        ''', (search_id,))
        
        articles = []
        for row in cursor.fetchall():
            article = {
                'pmid': row[2],
                'title': row[3],
                'journal': row[4],
                'journal_abbr': row[5],
                'year': row[6],
                'volume': row[7],
                'issue': row[8],
                'pages': row[9],
                'doi': row[10],
                'abstract': row[11],
                'authors': json.loads(row[12]) if row[12] else [],
                'article_types': json.loads(row[13]) if row[13] else [],
                'keywords': json.loads(row[14]) if row[14] else [],
                'citation': row[15],
                'pubmed_url': row[16],
                'impact_factor': row[17],
                'score': row[18]
            }
            articles.append(article)
        
        conn.close()
        
        return {
            'search_info': search_info,
            'articles': articles
        }
        
    except Exception as e:
        print(f"âŒ è·å–æœç´¢ç»“æœå¤±è´¥: {e}")
        return None

def display_articles_paginated(articles, page_size=50):
    """åˆ†é¡µæ˜¾ç¤ºæ–‡ç« ï¼ˆå‘½ä»¤è¡Œç‰ˆæœ¬ï¼‰"""
    if not articles:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„æ–‡ç« ")
        return
    
    total_articles = len(articles)
    total_pages = (total_articles + page_size - 1) // page_size
    
    print(f"\nğŸ“š æ‰¾åˆ° {total_articles} ç¯‡æ–‡ç«  (å·²æŒ‰è¯„åˆ†æ’åº)")
    print(f"ğŸ“„ å°†ä»¥æ¯é¡µ {page_size} ç¯‡çš„æ–¹å¼å±•ç¤ºï¼Œå…± {total_pages} é¡µ")
    print("="*100)
    
    current_page = 1
    
    while current_page <= total_pages:
        start_idx = (current_page - 1) * page_size
        end_idx = min(start_idx + page_size, total_articles)
        page_articles = articles[start_idx:end_idx]
        
        print(f"\nğŸ“– ç¬¬ {current_page}/{total_pages} é¡µ (ç¬¬ {start_idx + 1}-{end_idx} ç¯‡æ–‡ç« )")
        print("="*100)
        
        for i, article in enumerate(page_articles, start_idx + 1):
            print(f"\nğŸ“„ æ–‡ç«  {i}:")
            print(f"  ğŸ“ æ ‡é¢˜: {article['title']}")
            print(f"  ğŸ‘¥ ä½œè€…: {', '.join(article['authors'][:3])}" +
                  (f", et al. ({len(article['authors'])} total)" if len(article['authors']) > 3 else ""))
            print(f"  ğŸ“° æœŸåˆŠ: {article['journal']} ({article['year']})")
            print(f"  ğŸ“Š å½±å“å› å­: {article.get('impact_factor', 'N/A')}")
            print(f"  â­ è¯„åˆ†: {article.get('score', 'N/A')}")
            
            article_types = article.get("article_types", [])
            if article_types:
                print(f"  ğŸ“‹ æ–‡ç« ç±»å‹: {', '.join(article_types[:3])}" +
                      (f" (+{len(article_types)-3} more)" if len(article_types) > 3 else ""))
            
            if article.get("doi"): 
                print(f"  ğŸ”— DOI: {article['doi']}")
            print(f"  ğŸŒ PubMed: {article['pubmed_url']}")
            
            abstract = article['abstract']
            print(f"  ğŸ“„ æ‘˜è¦: {abstract[:200] + '...' if len(abstract) > 200 else abstract}")
            print("-" * 80)
        
        if current_page < total_pages:
            print(f"\nğŸ“„ å½“å‰æ˜¾ç¤ºç¬¬ {current_page}/{total_pages} é¡µ")
            choice = input("è¯·é€‰æ‹©æ“ä½œ (n=ä¸‹ä¸€é¡µ, p=ä¸Šä¸€é¡µ, j=è·³è½¬åˆ°æŒ‡å®šé¡µ, q=é€€å‡ºæµè§ˆ): ").lower().strip()
            
            if choice == 'n':
                current_page += 1
            elif choice == 'p' and current_page > 1:
                current_page -= 1
            elif choice == 'j':
                try:
                    target_page = int(input(f"è¯·è¾“å…¥é¡µç  (1-{total_pages}): "))
                    if 1 <= target_page <= total_pages:
                        current_page = target_page
                    else:
                        print("âŒ é¡µç è¶…å‡ºèŒƒå›´")
                except ValueError:
                    print("âŒ è¯·è¾“å…¥æœ‰æ•ˆçš„é¡µç ")
            elif choice == 'q':
                break
            else:
                print("âŒ æ— æ•ˆé€‰æ‹©ï¼Œè¯·é‡æ–°è¾“å…¥")
        else:
            print(f"\nâœ… å·²æ˜¾ç¤ºå®Œæ‰€æœ‰ {total_articles} ç¯‡æ–‡ç« ")
            break

def save_to_markdown(articles, filename, query, journal_query_info, year_range):
    """ä¿å­˜ç»“æœä¸ºMarkdownæ ¼å¼"""
    if not articles: 
        print("âŒ æ²¡æœ‰æ–‡ç« å¯ä»¥ä¿å­˜")
        return False
    
    try:
        with open(filename, 'w', encoding='utf-8') as f:
            f.write(f"# PubMedæœç´¢ç»“æœ\n\n")
            f.write(f"**æœç´¢å…³é”®è¯**: `{query}`  \n")
            if journal_query_info: 
                f.write(f"**æœŸåˆŠè¿‡æ»¤**: {journal_query_info}  \n")
            if year_range: 
                f.write(f"**å¹´ä»½èŒƒå›´**: {year_range}  \n")
            f.write(f"**æœç´¢æ—¥æœŸ**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}  \n")
            f.write(f"**ç»“æœæ•°é‡**: {len(articles)}ç¯‡æ–‡ç«   \n\n")
            
            f.write("## ç›®å½•\n\n")
            for i, article in enumerate(articles, 1):
                anchor = re.sub(r'[^\w\s-]', '', article['title'].lower().replace(' ', '-'))
                anchor = re.sub(r'[-\s]+', '-', anchor)[:50]
                f.write(f"{i}. [{article['title']}](#{anchor})  \n")
            
            f.write("\n---\n\n## æ–‡ç« è¯¦æƒ…\n\n")
            for i, article in enumerate(articles, 1):
                anchor = re.sub(r'[^\w\s-]', '', article['title'].lower().replace(' ', '-'))
                anchor = re.sub(r'[-\s]+', '-', anchor)[:50]
                f.write(f"### {i}. {article['title']} <a id='{anchor}'></a>\n\n")
                f.write("| é¡¹ç›® | å†…å®¹ |\n| --- | --- |\n")
                f.write(f"| æœŸåˆŠ | {article['journal']} |\n")
                f.write(f"| å½±å“å› å­ | {article.get('impact_factor', 'N/A')} |\n")
                f.write(f"| å¹´ä»½ | {article['year']} |\n")
                if article.get("volume"): 
                    f.write(f"| å·å· | {article['volume']} |\n")
                if article.get("issue"): 
                    f.write(f"| æœŸå· | {article['issue']} |\n")
                if article.get("pages"): 
                    f.write(f"| é¡µç  | {article['pages']} |\n")
                f.write(f"| è¯„åˆ† | {article.get('score', 'N/A')} |\n\n")
                f.write(f"**ä½œè€…**: {', '.join(article['authors'])}\n\n")
                if article.get("article_types"): 
                    f.write(f"**æ–‡ç« ç±»å‹**: {', '.join(article['article_types'])}\n\n")
                if article.get("keywords"): 
                    f.write(f"**å…³é”®è¯**: {', '.join(article['keywords'])}\n\n")
                f.write(f"**æ‘˜è¦**:  \n{article['abstract']}\n\n")
                f.write("**é“¾æ¥**:  \n")
                if article.get("doi"): 
                    f.write(f"  - DOI: [{article['doi']}](https://doi.org/{article['doi']})  \n")
                f.write(f"  - PubMed: [{article['pmid']}]({article['pubmed_url']})  \n\n")
                f.write(f"**å¼•ç”¨æ ¼å¼**:  \n```\n{article['citation']}\n```\n\n---\n\n")
            
            f.write("## æ³¨é‡Š\n\n")
            f.write("* æ­¤æœç´¢ç»“æœç”±AIå¢å¼ºå‹PubMedæœç´¢å·¥å…·è‡ªåŠ¨ç”Ÿæˆã€‚\n")
            f.write("* è¯„åˆ†ç³»ç»ŸåŸºäºæœŸåˆŠå½±å“å› å­ã€æ–‡ç« ç±»å‹ã€å‘è¡¨å¹´ä»½ç­‰å› ç´ ã€‚\n")
            f.write("* é»˜è®¤ä»…åŒ…å«é¢„å®šä¹‰çš„ä¸»åˆŠæ–‡ç« ã€‚\n")
        
        print(f"âœ… ç»“æœå·²ä¿å­˜åˆ° {filename}")
        return True
        
    except Exception as e: 
        print(f"âŒ ä¿å­˜Markdownæ–‡ä»¶æ—¶å‡ºé”™: {e}")
        return False

def save_to_json(articles, filename, query, journal_query_info, year_range):
    """ä¿å­˜ç»“æœä¸ºJSONæ ¼å¼"""
    if not articles: 
        print("âŒ æ²¡æœ‰æ–‡ç« å¯ä»¥ä¿å­˜")
        return False
    
    try:
        json_data = {
            "search_parameters": {
                "query": query, 
                "journal_filter": journal_query_info, 
                "year_range": year_range,
                "timestamp": datetime.now().strftime('%Y-%m-%d %H:%M:%S'), 
                "result_count": len(articles)
            }, 
            "articles": articles
        }
        
        with open(filename, 'w', encoding='utf-8') as f: 
            json.dump(json_data, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… JSONç»“æœå·²ä¿å­˜åˆ° {filename}")
        return True
        
    except Exception as e: 
        print(f"âŒ ä¿å­˜JSONæ–‡ä»¶æ—¶å‡ºé”™: {e}")
        return False

# ä¸»æœç´¢å‡½æ•°ï¼ˆå‘½ä»¤è¡Œç‰ˆæœ¬ï¼‰
def search_and_filter_pubmed():
    """
    ä¸»å‡½æ•°ï¼šæœç´¢å’Œè¿‡æ»¤PubMedæ–‡ç« ï¼Œå¸¦AIæŸ¥è¯¢ç”Ÿæˆé€‰é¡¹
    """
    # åˆå§‹åŒ–æ•°æ®åº“
    init_database()
    
    print("ğŸš€ æ¬¢è¿ä½¿ç”¨AIå¢å¼ºå‹PubMedæœç´¢å·¥å…· - ä¸»åˆŠæ–‡ç« ç‰ˆ!")
    print("="*100)
    print("ğŸ“š æœ¬å·¥å…·å°†ä¼˜å…ˆæœç´¢é¢„å®šä¹‰çš„31ç§ä¸»åˆŠæ–‡ç«  (å¦‚Nature, Science, Cell, Lancetç­‰)ã€‚")
    print("ğŸ¤– æ‚¨å¯ä»¥è¾“å…¥è‡ªç„¶è¯­è¨€ä¸»é¢˜ï¼Œç”±AIç”ŸæˆPubMedæŸ¥è¯¢ï¼Œæˆ–æ‰‹åŠ¨è¾“å…¥æŸ¥è¯¢ã€‚")
    print("ğŸ“Š è¯„åˆ†ç³»ç»ŸåŸºäºæœŸåˆŠå½±å“å› å­(IF)ï¼Œæ”¯æŒMarkdownå’ŒJSONæ ¼å¼å¯¼å‡ºã€‚")
    print("ğŸ’¾ æ‰€æœ‰æœç´¢è®°å½•å°†è‡ªåŠ¨ä¿å­˜åˆ°æœ¬åœ°æ•°æ®åº“ã€‚")
    print("="*100)

    # æ˜¾ç¤ºæœç´¢å†å²é€‰é¡¹
    history_choice = input("æ˜¯å¦æŸ¥çœ‹æœ€è¿‘çš„æœç´¢å†å²? (y/n, é»˜è®¤n): ").lower().strip()
    if history_choice == "y":
        show_search_history()
        print()

    query = ""
    user_topic = ""
    ai_generated_query = ""
    
    use_ai = input("æ˜¯å¦ä½¿ç”¨AIæ ¹æ®æ‚¨çš„ä¸»é¢˜ç”ŸæˆPubMedæŸ¥è¯¢? (y/n, é»˜è®¤y): ").lower().strip()
    if use_ai == "" or use_ai == "y":
        user_topic = input("è¯·è¾“å…¥æ‚¨çš„ç ”ç©¶ä¸»é¢˜ (è‡ªç„¶è¯­è¨€æè¿°ï¼Œä¾‹å¦‚ï¼šç«¯ç²’é•¿åº¦ä¸è¡°è€å’Œé•¿å¯¿çš„å…³ç³»ç ”ç©¶): ")
        if user_topic:
            ai_generated_query = generate_pubmed_query_with_ai(user_topic)
            if ai_generated_query:
                print("\nğŸ’¡ AIå»ºè®®çš„PubMedæŸ¥è¯¢:")
                print(f"   {ai_generated_query}")
                while True:
                    confirm_ai_query = input("æ˜¯å¦ä½¿ç”¨æ­¤æŸ¥è¯¢? (y: ä½¿ç”¨ / e: ç¼–è¾‘ / n: æ‰‹åŠ¨è¾“å…¥): ").lower().strip()
                    if confirm_ai_query == 'y':
                        query = ai_generated_query
                        break
                    elif confirm_ai_query == 'e':
                        edited_query = input(f"è¯·ç¼–è¾‘æŸ¥è¯¢ (å½“å‰: {ai_generated_query}): ")
                        query = edited_query if edited_query.strip() else ai_generated_query
                        break
                    elif confirm_ai_query == 'n':
                        query = ""
                        break
                    else:
                        print("âŒ æ— æ•ˆé€‰æ‹©ï¼Œè¯·è¾“å…¥ y, e, æˆ– nã€‚")
            else:
                print("âŒ AIæœªèƒ½ç”ŸæˆæŸ¥è¯¢ï¼Œè¯·æ‰‹åŠ¨è¾“å…¥ã€‚")
        else:
            print("âŒ æœªæä¾›ç ”ç©¶ä¸»é¢˜ï¼Œè¯·æ‰‹åŠ¨è¾“å…¥æŸ¥è¯¢ã€‚")

    if not query:
        query = input("è¯·è¾“å…¥PubMedæœç´¢å…³é”®è¯ (è‹±æ–‡æ•ˆæœæ›´å¥½ï¼Œæ”¯æŒå¸ƒå°”é€»è¾‘): ")

    if not query.strip():
        print("âŒ é”™è¯¯ï¼šæœªæä¾›ä»»ä½•æœç´¢æŸ¥è¯¢ã€‚ç¨‹åºé€€å‡ºã€‚")
        return

    default_journals_display = "Nature, Science, Cell, The Lancet, JAMA, NEJM, BMJ"
    journal_input = input(f"è¯·è¾“å…¥ç›®æ ‡æœŸåˆŠ (å¤šä¸ªç”¨é€—å·åˆ†éš”ï¼Œé»˜è®¤æœç´¢æ‰€æœ‰é¢„å®šä¹‰ä¸»åˆŠ): \n(ç¤ºä¾‹: {default_journals_display}): ")
    min_year = input("è¯·è¾“å…¥æœ€æ—©å¹´ä»½ (å¯é€‰ï¼Œå›è½¦è·³è¿‡): ").strip()
    max_year = input("è¯·è¾“å…¥æœ€æ™šå¹´ä»½ (å¯é€‰ï¼Œå›è½¦è·³è¿‡): ").strip()
    
    try:
        min_score_input = input("è¯·è¾“å…¥æœ€ä½åˆ†æ•°è¿‡æ»¤ (å¯é€‰ï¼Œå›è½¦è·³è¿‡ï¼Œé»˜è®¤0ä¸è¿‡æ»¤): ").strip()
        min_score = float(min_score_input) if min_score_input else 0.0
    except ValueError: 
        min_score = 0.0
        print("âŒ æœ€ä½åˆ†æ•°è¾“å…¥æ— æ•ˆï¼Œä¸è¿›è¡Œåˆ†æ•°è¿‡æ»¤ã€‚")

    year_range_str = ""
    if min_year and max_year: 
        year_range_str = f"{min_year}-{max_year}"
    elif min_year: 
        year_range_str = f"{min_year}ä»¥å"
    elif max_year: 
        year_range_str = f"{max_year}ä»¥å‰"
    
    main_journals_only_flag = True
    print("\nğŸ” å°†ä¸¥æ ¼ç­›é€‰é¢„å®šä¹‰çš„ä¸»åˆŠæ–‡ç« ã€‚")
    journal_query_display = journal_input if journal_input else "æ‰€æœ‰é¢„å®šä¹‰ä¸»åˆŠ"

    # å¼€å§‹æœç´¢
    print("\nğŸš€ å¼€å§‹æœç´¢...")
    search_session = search_pubmed(query, journal_input, min_year, max_year, main_journals_only_flag)
    
    if not search_session["pmids"] and not (search_session["web_env"] and search_session["query_key"]):
        print("âŒ åˆæ­¥æœç´¢æœªè¿”å›ä»»ä½•PMIDæˆ–æœ‰æ•ˆçš„æœç´¢ä¼šè¯ã€‚è¯·å°è¯•æ”¾å®½æœç´¢æ¡ä»¶ã€‚")
        return

    total_found = search_session.get("total_count", 0)
    print(f"ğŸ“Š æœç´¢å®Œæˆï¼æ‰¾åˆ° {total_found} ç¯‡ç›¸å…³æ–‡ç« ")
    
    if total_found == 0:
        print("âŒ æœªæ‰¾åˆ°ä»»ä½•æ–‡ç« ï¼Œè¯·å°è¯•å…¶ä»–æœç´¢æ¡ä»¶ã€‚")
        return

    # è·å–æ–‡ç« è¯¦æƒ…
    articles_detailed = fetch_article_details(
        pmids=search_session.get("pmids"), 
        web_env=search_session.get("web_env"),
        query_key=search_session.get("query_key"), 
        main_journals_only=main_journals_only_flag
    )
    
    if not articles_detailed:
        print("âŒ æœªèƒ½è·å–åˆ°ä»»ä½•ç¬¦åˆä¸»åˆŠæ¡ä»¶çš„æ–‡ç« è¯¦ç»†ä¿¡æ¯ã€‚")
        return

    print(f"âœ… æˆåŠŸè·å– {len(articles_detailed)} ç¯‡ä¸»åˆŠæ–‡ç« çš„è¯¦ç»†ä¿¡æ¯")

    # è¯„åˆ†å’Œè¿‡æ»¤
    scored_articles = assign_scores_by_if(articles_detailed)
    final_articles = filter_articles(scored_articles, min_score)
    
    if not final_articles:
        print("âŒ ç»è¿‡æ‰€æœ‰ç­›é€‰åï¼Œæ²¡æœ‰æ–‡ç« å¯ä¾›æ˜¾ç¤ºã€‚")
        return

    # ä¿å­˜æœç´¢å‚æ•°ç”¨äºæ•°æ®åº“å­˜å‚¨
    search_params = {
        'user_topic': user_topic,
        'ai_generated_query': ai_generated_query,
        'final_query': query,
        'journal_filter': journal_query_display,
        'year_range': year_range_str,
        'min_score': min_score,
        'total_results': total_found
    }

    # ä¿å­˜åˆ°æ•°æ®åº“
    search_id = save_search_to_database(search_params, final_articles)

    # åˆ†é¡µæ˜¾ç¤ºæ–‡ç« 
    display_articles_paginated(final_articles, page_size=50)

    # ä¿å­˜æ–‡ä»¶é€‰é¡¹
    print("\nğŸ’¾ è¯·é€‰æ‹©ä¿å­˜æ ¼å¼:")
    print("1. Markdown (.md)  2. JSON (.json)  3. çº¯æ–‡æœ¬ (.txt)  4. Markdownå’ŒJSON  5. ä¸ä¿å­˜")
    save_choice = input("è¯·é€‰æ‹© (é»˜è®¤1): ") or "1"
    
    results_dir = "pubmed_results"
    if not os.path.exists(results_dir): 
        os.makedirs(results_dir)
    
    timestamp_str = datetime.now().strftime("%Y%m%d_%H%M%S")
    base_filename = f"pubmed_search_{timestamp_str}"

    if save_choice in ["1", "4"]:
        md_file = os.path.join(results_dir, f"{base_filename}.md")
        save_to_markdown(final_articles, md_file, query, journal_query_display, year_range_str)
    
    if save_choice in ["2", "4"]:
        json_file = os.path.join(results_dir, f"{base_filename}.json")
        save_to_json(final_articles, json_file, query, journal_query_display, year_range_str)
    
    if save_choice == "3":
        txt_file = os.path.join(results_dir, f"{base_filename}.txt")
        try:
            with open(txt_file, 'w', encoding='utf-8') as f:
                f.write(f"PubMedæœç´¢ç»“æœ - å…³é”®è¯: {query}\næœŸåˆŠè¿‡æ»¤: {journal_query_display}\n")
                if year_range_str: 
                    f.write(f"å¹´ä»½: {year_range_str}\n")
                f.write("="*80 + "\n\n")
                for i, art in enumerate(final_articles, 1):
                    f.write(f"æ–‡ç«  {i}:\n  æ ‡é¢˜: {art['title']}\n  ä½œè€…: {', '.join(art['authors'])}\n")
                    f.write(f"  æœŸåˆŠ: {art['journal']}, {art['year']}\n")
                    f.write(f"  IF: {art.get('impact_factor', 'N/A')}, Score: {art.get('score', 'N/A')}\n")
                    f.write(f"  æ‘˜è¦: {art['abstract'][:300]}...\n")
                    f.write(f"  PMID: {art['pmid']}, DOI: {art.get('doi', 'N/A')}\n")
                    f.write(f"  Link: {art['pubmed_url']}\n" + "-"*40 + "\n\n")
            print(f"âœ… æ–‡æœ¬æ–‡ä»¶å·²ä¿å­˜åˆ°: {txt_file}")
        except Exception as e: 
            print(f"âŒ ä¿å­˜æ–‡æœ¬æ–‡ä»¶æ—¶å‡ºé”™: {e}")
    
    if save_choice == "5": 
        print("ğŸ“ ç»“æœæœªä¿å­˜åˆ°æ–‡ä»¶ã€‚")
    
    # å¯é€‰ï¼šæ˜¾ç¤ºJSONæ ¼å¼
    show_json_console = input("\næ˜¯å¦åœ¨æ§åˆ¶å°æ˜¾ç¤ºJSONæ ¼å¼çš„ç»“æœ (å‰2ç¯‡)? (y/n): ").lower()
    if show_json_console == 'y':
        print("\nğŸ“„ JSONæ ¼å¼é¢„è§ˆ (å‰2ç¯‡):")
        print(json.dumps(final_articles[:min(2, len(final_articles))], ensure_ascii=False, indent=2))

    print(f"\nğŸ‰ æœç´¢å®Œæˆï¼æœ¬æ¬¡æœç´¢å·²ä¿å­˜åˆ°æ•°æ®åº“ (ID: {search_id})")
    print("æ„Ÿè°¢ä½¿ç”¨AIå¢å¼ºå‹PubMedæœç´¢å·¥å…·ï¼")

def show_search_history():
    """æ˜¾ç¤ºæœç´¢å†å²"""
    try:
        conn = sqlite3.connect(DATABASE_PATH)
        cursor = conn.cursor()
        
        cursor.execute('''
            SELECT id, search_date, user_topic, final_query, journal_filter, 
                   year_range, filtered_results 
            FROM search_history 
            ORDER BY created_at DESC 
            LIMIT 10
        ''')
        
        history = cursor.fetchall()
        conn.close()
        
        if not history:
            print("ğŸ“ æš‚æ— æœç´¢å†å²")
            return
            
        print("\nğŸ“ æœ€è¿‘10æ¬¡æœç´¢å†å²:")
        print("="*100)
        for record in history:
            search_id, date, topic, query, journal, year_range, results = record
            print(f"ID: {search_id} | æ—¥æœŸ: {date} | ç»“æœæ•°: {results}")
            if topic:
                print(f"  ä¸»é¢˜: {topic}")
            print(f"  æŸ¥è¯¢: {query}")
            if journal:
                print(f"  æœŸåˆŠ: {journal}")
            if year_range:
                print(f"  å¹´ä»½: {year_range}")
            print("-" * 100)
            
    except Exception as e:
        print(f"âŒ è·å–æœç´¢å†å²å¤±è´¥: {e}")

if __name__ == "__main__":
    search_and_filter_pubmed()
